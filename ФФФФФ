# **–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫**

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü:

import requests
from bs4 import BeautifulSoup
import time
import re

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF:

!pip install pymupdf
import fitz

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É:

import string

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ñ–∏–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

!pip install stanza

import stanza
stanza.download('fi')
nlp = stanza.Pipeline('fi', processors='tokenize,pos,lemma')

from collections import Counter

import matplotlib.pyplot as plt
import numpy as np

# **–í—ã–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞**

P.S. –í—ã–≥—Ä—É–∑–∫–∞ —Å —Å–∞–π—Ç–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ 5 –º–∏–Ω—É—Ç, —Ç.–∫. –≤ —Ç–µ–∫—Å—Ç–µ –º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü

def get_text(url):
    response = requests.get(url)
    response.encoding = "utf-8"
    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
        return ""

    soup = BeautifulSoup(response.text, "html.parser")
    text_blocks = soup.find_all("br")
    text = ""
    for br in text_blocks:
        if br.next_sibling and isinstance(br.next_sibling, str):
            line = br.next_sibling.strip()
            if not re.search(r'\(1918\)|H√§r nedan syns|mode=normal|https://runeberg.org/ahvenkulta/\d{4}\.html', line):
                text += line + "\n"

    return text.strip()

def scrape_book(base_url, start_page, end_page):
    all_text = ""

    for i in range(start_page, end_page + 1, 1):
        page_number = str(i).zfill(4)
        url = f"{base_url}{page_number}.html"
        page_text = get_text(url)
        all_text += page_text + "\n\n"
        time.sleep(1)

    return all_text

if __name__ == "__main__":
    base_url = "https://runeberg.org/ahvenkulta/"
    start_page = 9   #–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–Ω–∏–≥–∏
    end_page = 142   #–ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞

    purefinnish_text = scrape_book(base_url, start_page, end_page)

    with open("purefinnish_text.txt", "w", encoding="utf-8") as file:
        file.write(purefinnish_text)

–ü–æ—Å—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –ø–æ–ª—É—á–∏–ª–æ—Å—å –≤ –∏—Ç–æ–≥–æ–≤–æ–º —Ñ–∞–π–ª–µ

with open("purefinnish_text.txt", "r", encoding="utf-8") as f:
    text = f.read()
words = text.split()
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {len(words)}")

# **–í—ã–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏—è–º–∏**

def loanwords_text(Gradu, skip_pages=None):
    text = ""
    skip_pages = set(skip_pages) if skip_pages else set()

    with fitz.open(Gradu) as doc:
        for page_num, page in enumerate(doc, start=1):
          if page_num in skip_pages:
            continue
          text += page.get_text("text") + "\n"
    return text

pdf_file = "Gradu.pdf"
skip_pages = {2, 11, 13} | set(range(69, 78))
loanwords_text = loanwords_text(pdf_file, skip_pages)

with open("loanwords_text.txt", "w", encoding="utf-8") as f:
    f.write(loanwords_text)

–£–±–∏—Ä–∞—é –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–ª–æ–≤–∞ –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ (–∞–Ω–≥–ª–∏–π—Å–∫–æ–º)

!pip install lingua-language-detector

from lingua import Language, LanguageDetectorBuilder

EXCEPTIONS = {"on", "enantiomeerin", "enantiomeeri",
              "enantiomeeria", "enantiomeerien", "polystyreenin",
              "prioriteetin", "bromi", "happea"}

def detect(word):
    if word.lower() in EXCEPTIONS:
        return "FINNISH", 1.0
    languages = [Language.ENGLISH, Language.FINNISH]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()

    confidence_values = detector.compute_language_confidence_values(word)
    finnish_confidence = next((x.value for x in confidence_values if x.language == Language.FINNISH), 0)
    return finnish_confidence >= 0.5

with open("loanwords_text.txt", "r", encoding="utf-8") as f:
    words = f.read().split()

finnish_words = [word for word in words if detect(word)]

with open("loanwords_text_finnishonly.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(finnish_words))

–ü–æ—Å—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –ø–æ–ª—É—á–∏–ª–æ—Å—å –≤ –∏—Ç–æ–≥–æ–≤–æ–º —Ñ–∞–π–ª–µ

with open("loanwords_text_finnishonly.txt", "r", encoding="utf-8") as f:
    text = f.read()
words = text.split()
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {len(words)}")

# **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤**

def preprocess_text(file_path, output_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()

    # –£–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä
    text = re.sub(r'\d+', '', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Å–∫–æ–±–∫–∞—Ö (–∫—Ä—É–≥–ª—ã—Ö –∏ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö)
    text = re.sub(r'\(.*?\)|\[.*?\]', '', text, flags=re.DOTALL)

    # –£–¥–∞–ª–µ–Ω–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä (—Å–ª–æ–≤ —Å —Ç–æ—á–∫–∞–º–∏)
    text = re.sub(r'\b(?:[a-z√§√∂√•]+\.){2,}\b', '', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'[^\w\s√§√∂√•]', '', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'\s+', ' ', text).strip()

    # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤
    words = text.split()
    seen = set()
    unique_words = []

    for word in words:
        if word not in seen and len(word) > 2:
          seen.add(word)
          unique_words.append(word)

    cleaned_text = ' '.join(unique_words)

    with open(output_path, 'w', encoding='utf-8') as output_file:
        output_file.write(cleaned_text)

    return len(unique_words)

input_files = ['purefinnish_text.txt', 'loanwords_text_finnishonly.txt']
output_files = ['clean_pure_text.txt', 'clean_loanwords_text.txt']

–ü–æ—Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö

def count_words(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        words = file.read().split()
        return len(words)

file1 = "clean_pure_text.txt"
file2 = "clean_loanwords_text.txt"

word_count1 = count_words(file1)
word_count2 = count_words(file2)

print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —á–∏—Å—Ç–æ —Ñ–∏–Ω—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ: {word_count1}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ —Å –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏—è–º–∏: {word_count2}")

!pip install selenium
!apt-get update 
!apt install chromium-chromedriver


# **–ß–ê–°–¢–û–¢–ù–´–ô –ê–ù–ê–õ–ò–ó**

## 1. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥–ª–∞—Å–Ω—ã—Ö

def all_vowels(vowel_file):
    with open(vowel_file, "r", encoding="utf-8") as f:
        vowels = f.read().strip().split()
    return vowels

def all_words (file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        words = file.read().lower().split()
    return words

def vowel_frequencies(file_path, finnish_vowels):
    vowels = all_vowels(finnish_vowels)
    words = all_words(file_path)
    front_vowels = {"√§", "√∂", "y"}
    back_vowels = {"a", "o", "u"}
    mid_vowels = {"e", "i"}

    vowel_counts = Counter()
    front_count, back_count = 0, 0
    start_vowel_counts = Counter()
    mid_vowel_counts = Counter()
    end_vowel_counts = Counter()

    for word in words:
        word_vowels = [ch for ch in word if ch in vowels]
        vowel_counts.update(word_vowels)

        if any(ch in front_vowels for ch in word_vowels):
            front_count += 1
        if any(ch in back_vowels for ch in word_vowels):
            back_count += 1

        if word_vowels:
            start_vowel_counts[word_vowels[0]] += 1
            if len(word_vowels) > 2:
                mid_vowel_counts[word_vowels[len(word_vowels)//2]] += 1
            end_vowel_counts[word_vowels[-1]] += 1
    return vowel_counts, front_count, back_count, start_vowel_counts, mid_vowel_counts, end_vowel_counts

file1 = "clean_pure_text.txt"   # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
file2 = "clean_loanwords_text.txt"  # –ó–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
vowel_file = "finnish_vowels.txt"

# –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –≥–ª–∞—Å–Ω—ã—Ö –≤ –æ–±–æ–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö
freq1, front1, back1, start1, mid1, end1 = vowel_frequencies(file1, vowel_file)
freq2, front2, back2, start2, mid2, end2 = vowel_frequencies(file2, vowel_file)

# –í—ã–≤–æ–¥ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö (–í–ï–†–ù–£–õ–ò –í–°–Å!)
print("\n–ß–∞—Å—Ç–æ—Ç–∞ –≥–ª–∞—Å–Ω—ã—Ö –≤ –¢–µ–∫—Å—Ç–µ 1:", freq1)
print("–ß–∞—Å—Ç–æ—Ç–∞ –≥–ª–∞—Å–Ω—ã—Ö –≤ –¢–µ–∫—Å—Ç–µ 2:", freq2)

print(f"\n–ü–µ—Ä–µ–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ: –¢–µ–∫—Å—Ç 1 -> {front1}, –¢–µ–∫—Å—Ç 2 -> {front2}")
print(f"–ó–∞–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ: –¢–µ–∫—Å—Ç 1 -> {back1}, –¢–µ–∫—Å—Ç 2 -> {back2}")

print("\n–ß–∞—Å—Ç–æ—Ç–∞ –≥–ª–∞—Å–Ω—ã—Ö –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞:")
print("–¢–µ–∫—Å—Ç 1:", start1)
print("–¢–µ–∫—Å—Ç 2:", start2)

print("\n–ß–∞—Å—Ç–æ—Ç–∞ –≥–ª–∞—Å–Ω—ã—Ö –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Å–ª–æ–≤–∞:")
print("–¢–µ–∫—Å—Ç 1:", mid1)
print("–¢–µ–∫—Å—Ç 2:", mid2)

print("\n–ß–∞—Å—Ç–æ—Ç–∞ –≥–ª–∞—Å–Ω—ã—Ö –≤ –∫–æ–Ω—Ü–µ —Å–ª–æ–≤–∞:")
print("–¢–µ–∫—Å—Ç 1:", end1)
print("–¢–µ–∫—Å—Ç 2:", end2)


## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –≥–ª–∞—Å–Ω—ã—Ö

def normalize_frequencies(freq, total_vowels):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã"""
    return {vowel: (count / total_vowels) * 100 for vowel, count in freq.items()}

# –°—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞—Å–Ω—ã—Ö –≤ –∫–∞–∂–¥–æ–º —Ç–µ–∫—Å—Ç–µ
total_vowels1 = sum(freq1.values())
total_vowels2 = sum(freq2.values())

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
freq1_percent = normalize_frequencies(freq1, total_vowels1)
freq2_percent = normalize_frequencies(freq2, total_vowels2)


def visualize_vowel_frequencies(freq1, freq2):
    """–°—Ç—Ä–æ–∏—Ç —Å—Ç–æ–ª–±—á–∞—Ç—ã–π –≥—Ä–∞—Ñ–∏–∫ —á–∞—Å—Ç–æ—Ç—ã –≥–ª–∞—Å–Ω—ã—Ö (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö) –ø–æ —É–±—ã–≤–∞–Ω–∏—é"""

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –∏–∑ –æ–±–æ–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    combined_freq = {vowel: freq1.get(vowel, 0) + freq2.get(vowel, 0) for vowel in set(freq1) | set(freq2)}

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ–±—â–µ–π —á–∞—Å—Ç–æ—Ç—ã
    sorted_vowels = sorted(combined_freq.keys(), key=lambda v: combined_freq[v], reverse=True)

    # –¢–µ–ø–µ—Ä—å –±–µ—Ä–µ–º —á–∞—Å—Ç–æ—Ç—ã –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è
    values1 = [freq1.get(vowel, 0) for vowel in sorted_vowels]
    values2 = [freq2.get(vowel, 0) for vowel in sorted_vowels]

    x = np.arange(len(sorted_vowels))

    plt.figure(figsize=(10, 6))
    width = 0.4  # –®–∏—Ä–∏–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤

    plt.bar(x - width/2, values1, width=width, label="–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç", alpha=0.7)
    plt.bar(x + width/2, values2, width=width, label="–ó–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞", alpha=0.7)

    plt.xlabel("–ì–ª–∞—Å–Ω—ã–µ")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞ (%)")
    plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –≥–ª–∞—Å–Ω—ã—Ö (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö, –ø–æ —É–±—ã–≤–∞–Ω–∏—é)")
    plt.xticks(ticks=x, labels=sorted_vowels)
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é
visualize_vowel_frequencies(freq1_percent, freq2_percent)

## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –≥–ª–∞—Å–Ω—ã—Ö *–ø–æ* —Ä—è–¥—É

# –°—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞—Å–Ω—ã—Ö –≤ –∫–∞–∂–¥–æ–º —Ç–µ–∫—Å—Ç–µ
total_vowels1 = sum(freq1.values())
total_vowels2 = sum(freq2.values())

# –ü–µ—Ä–µ–¥–Ω–∏–µ, —Å—Ä–µ–¥–Ω–∏–µ –∏ –∑–∞–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ!)
front1_total = sum(freq1[v] for v in ["√§", "√∂", "y"] if v in freq1)
mid1_total = sum(freq1[v] for v in ["e", "i"] if v in freq1)
back1_total = sum(freq1[v] for v in ["a", "o", "u"] if v in freq1)

front2_total = sum(freq2[v] for v in ["√§", "√∂", "y"] if v in freq2)
mid2_total = sum(freq2[v] for v in ["e", "i"] if v in freq2)
back2_total = sum(freq2[v] for v in ["a", "o", "u"] if v in freq2)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
front1_percent = (front1_total / total_vowels1) * 100
mid1_percent = (mid1_total / total_vowels1) * 100
back1_percent = (back1_total / total_vowels1) * 100

front2_percent = (front2_total / total_vowels2) * 100
mid2_percent = (mid2_total / total_vowels2) * 100
back2_percent = (back2_total / total_vowels2) * 100


def front_mid_back_bar_chart(front1, mid1, back1, front2, mid2, back2):
    """–°—Ç—Ä–æ–∏—Ç —Å—Ç–æ–ª–±—á–∞—Ç—ã–π –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –ø–µ—Ä–µ–¥–Ω–∏—Ö, —Å—Ä–µ–¥–Ω–∏—Ö –∏ –∑–∞–¥–Ω–∏—Ö –≥–ª–∞—Å–Ω—ã—Ö (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)"""
    categories = ["–ü–µ—Ä–µ–¥–Ω–∏–µ", "–°—Ä–µ–¥–Ω–∏–µ", "–ó–∞–¥–Ω–∏–µ"]
    values1 = [front1, mid1, back1]
    values2 = [front2, mid2, back2]

    x = np.arange(len(categories))

    plt.figure(figsize=(8, 5))
    plt.bar(x - 0.2, values1, width=0.4, label="–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç", alpha=0.7)
    plt.bar(x + 0.2, values2, width=0.4, label="–ó–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞", alpha=0.7)

    plt.xticks(ticks=x, labels=categories)
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞ (%)")
    plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–Ω–∏—Ö, —Å—Ä–µ–¥–Ω–∏—Ö –∏ –∑–∞–¥–Ω–∏—Ö –≥–ª–∞—Å–Ω—ã—Ö (%)")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
front_mid_back_bar_chart(front1_percent, mid1_percent, back1_percent,
                         front2_percent, mid2_percent, back2_percent)


# **–ë–ï–ó –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–ò**

# –ê–Ω–∞–ª–∏–∑ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
import re
from collections import Counter

def load_vowels(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        lines = [line.strip() for line in file.readlines() if line.strip()]

    if len(lines) < 3:
        return {
            "front": {"√§", "√∂", "y"},  # –ü–µ—Ä–µ–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ
            "back": {"a", "o", "u"},   # –ó–∞–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ
            "neutral": {"e", "i"}       # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≥–ª–∞—Å–Ω—ã–µ
        }

    vowels = {
        "front": set(lines[0]),  # –ü–µ—Ä–µ–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ (√§, √∂, y)
        "back": set(lines[1]),   # –ó–∞–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ (a, o, u)
        "neutral": set(lines[2])  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≥–ª–∞—Å–Ω—ã–µ (e, i)
    }
    return vowels

def check_vowel_harmony(word, vowels):
    front_vowel = any(char in vowels["front"] for char in word)
    back_vowel = any(char in vowels["back"] for char in word)
    return not (front_vowel and back_vowel)  # True, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –≥–∞—Ä–º–æ–Ω–∏—á–Ω–æ

def analyze_text_for_harmony(text, vowels):
    words = text.split()
    disharmonic_words = [word for word in words if not check_vowel_harmony(word, vowels)]
    return disharmonic_words

def process_and_analyze(input_file, vowels_file, output_file):
    with open(input_file, "r", encoding="utf-8") as file:
        text = file.read()

    vowels = load_vowels(vowels_file)
    disharmonic_words = analyze_text_for_harmony(text, vowels)

    with open(output_file, "w", encoding="utf-8") as file:
        file.write("\n".join(disharmonic_words))

    print(f"–§–∞–π–ª {output_file} —Å–æ–¥–µ—Ä–∂–∏—Ç {len(disharmonic_words)} —Å–ª–æ–≤, –Ω–∞—Ä—É—à–∞—é—â–∏—Ö —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º.")

def main():
    vowels_file = "finnish_vowels.txt"
    process_and_analyze("clean_pure_text.txt", vowels_file, "pure_finnish_harmony_no_lemm.txt")
    process_and_analyze("clean_loanwords_text.txt", vowels_file, "borrowed_finnish_harmony_no_lemm.txt")

if __name__ == "__main__":
    main()

# –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

def calculate_harmony_percentage(input_file, disharmonic_file):
    with open(input_file, "r", encoding="utf-8") as file:
        words = file.read().split()

    with open(disharmonic_file, "r", encoding="utf-8") as file:
        disharmonic_words = file.read().splitlines()

    total_words = len(words)
    disharmonic_count = len(disharmonic_words)
    harmonic_count = total_words - disharmonic_count

    if total_words > 0:
        harmonic_percentage = (harmonic_count / total_words) * 100
        disharmonic_percentage = (disharmonic_count / total_words) * 100
    else:
        harmonic_percentage = disharmonic_percentage = 0

    print(f"–§–∞–π–ª {input_file}:")
    print(f"–ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞: {harmonic_count} ({harmonic_percentage:.2f}%)")
    print(f"–ù–∞—Ä—É—à–∞—é—â–∏–µ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º: {disharmonic_count} ({disharmonic_percentage:.2f}%)")
    print("-" * 50)

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
files = [
    ("clean_pure_text.txt", "pure_finnish_harmony_no_lemm.txt"),
    ("clean_loanwords_text.txt", "borrowed_finnish_harmony_no_lemm.txt")
]

# –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
for input_file, disharmonic_file in files:
    calculate_harmony_percentage(input_file, disharmonic_file)


# **–° –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–ï–ô**

def lemmatize_finnish_file(input_file, output_file):
    with open(input_file, "r", encoding="utf-8") as f:
        text = f.read()
    doc = nlp(text)

    lemmatized_text = ' '.join([word.lemma for sentence in doc.sentences for word in sentence.words])

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(lemmatized_text)

input_files = ['clean_pure_text.txt', 'clean_loanwords_text.txt']
output_files = ['lemmatized_pure.txt', 'lemmatized_loanwords.txt']

for inp, out in zip(input_files, output_files):
    lemmatize_finnish_file(inp, out)


–û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

def process_sorted_unique_words(input_file, output_file):
    with open(input_file, "r", encoding="utf-8") as f:
        words = f.read().split()

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    unique_sorted_words = sorted(set(words))

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω–æ–≤—ã–π —Ñ–∞–π–ª
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(" ".join(unique_sorted_words))

    print(f'–§–∞–π–ª {output_file} —Å–æ–¥–µ—Ä–∂–∏—Ç {len(unique_sorted_words)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤')

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
input_files = ['lemmatized_pure.txt', 'lemmatized_loanwords.txt']
output_files = ['sorted_pure_finnish.txt', 'sorted_text2.txt']

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
for inp, out in zip(input_files, output_files):
    process_sorted_unique_words(inp, out)


–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º

import re
from collections import Counter

def load_vowels(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        lines = [line.strip() for line in file.readlines() if line.strip()]

    if len(lines) < 3:
        return {
            "front": {"√§", "√∂", "y"},  # –ü–µ—Ä–µ–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ
            "back": {"a", "o", "u"},   # –ó–∞–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ
            "neutral": {"e", "i"}       # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≥–ª–∞—Å–Ω—ã–µ
        }

    vowels = {
        "front": set(lines[0]),  # –ü–µ—Ä–µ–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ (√§, √∂, y)
        "back": set(lines[1]),   # –ó–∞–¥–Ω–∏–µ –≥–ª–∞—Å–Ω—ã–µ (a, o, u)
        "neutral": set(lines[2])  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≥–ª–∞—Å–Ω—ã–µ (e, i)
    }
    return vowels

def check_vowel_harmony(word, vowels):
    front_vowel = any(char in vowels["front"] for char in word)
    back_vowel = any(char in vowels["back"] for char in word)
    return not (front_vowel and back_vowel)  # True, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –≥–∞—Ä–º–æ–Ω–∏—á–Ω–æ

def analyze_text_for_harmony(text, vowels):
    words = text.split()
    disharmonic_words = [word for word in words if not check_vowel_harmony(word, vowels)]
    return disharmonic_words

def process_and_analyze(input_file, vowels_file, output_file):
    with open(input_file, "r", encoding="utf-8") as file:
        text = file.read()

    vowels = load_vowels(vowels_file)
    disharmonic_words = analyze_text_for_harmony(text, vowels)

    with open(output_file, "w", encoding="utf-8") as file:
        file.write("\n".join(disharmonic_words))

    print(f"–§–∞–π–ª {output_file} —Å–æ–¥–µ—Ä–∂–∏—Ç {len(disharmonic_words)} —Å–ª–æ–≤, –Ω–∞—Ä—É—à–∞—é—â–∏—Ö —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º.")

def main():
    vowels_file = "finnish_vowels.txt"
    process_and_analyze("sorted_pure_finnish.txt", vowels_file, "pure_finnish_harmony.txt")
    process_and_analyze("sorted_text2.txt", vowels_file, "borrowed_finnish_harmony.txt")

if __name__ == "__main__":
    main()

–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

def calculate_harmony_percentage(input_file, disharmonic_file):
    with open(input_file, "r", encoding="utf-8") as file:
        words = file.read().split()

    with open(disharmonic_file, "r", encoding="utf-8") as file:
        disharmonic_words = file.read().splitlines()

    total_words = len(words)
    disharmonic_count = len(disharmonic_words)
    harmonic_count = total_words - disharmonic_count

    if total_words > 0:
        harmonic_percentage = (harmonic_count / total_words) * 100
        disharmonic_percentage = (disharmonic_count / total_words) * 100
    else:
        harmonic_percentage = disharmonic_percentage = 0

    print(f"–§–∞–π–ª {input_file}:")
    print(f"–ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞: {harmonic_count} ({harmonic_percentage:.2f}%)")
    print(f"–ù–∞—Ä—É—à–∞—é—â–∏–µ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º: {disharmonic_count} ({disharmonic_percentage:.2f}%)")
    print("-" * 50)

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
files = [
    ("sorted_pure_finnish.txt", "pure_finnish_harmony.txt"),
    ("sorted_text2.txt", "borrowed_finnish_harmony.txt")
]

# –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
for input_file, disharmonic_file in files:
    calculate_harmony_percentage(input_file, disharmonic_file)


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

import matplotlib.pyplot as plt
import numpy as np

def load_statistics(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ —Ñ–∞–π–ª–∞ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –æ—à–∏–±–∫–∏."""
    stats = {}
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            lines = file.readlines()
        
        for line in lines:
            parts = line.strip().split(":")
            if len(parts) == 2:
                key, value = parts[0].strip(), parts[1].strip()
                try:
                    if "%" in value:
                        stats[key] = float(value.replace("%", ""))
                    else:
                        stats[key] = int(value) if value.isdigit() else 0
                except ValueError:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏: {line.strip()}")
    except FileNotFoundError:
        print(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
    
    return stats

# –§–∞–π–ª—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
stats_files = {
    "–ë–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (—á–∏—Å—Ç–æ —Ñ–∏–Ω—Å–∫–∏–π)": "pure_finnish_harmony_no_lemm.txt",
    "–ë–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (—Å –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏—è–º–∏)": "borrowed_finnish_harmony_no_lemm.txt",
    "–° –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π (—á–∏—Å—Ç–æ —Ñ–∏–Ω—Å–∫–∏–π)": "pure_finnish_harmony_lemma.txt",
    "–° –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π (—Å –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏—è–º–∏)": "borrowed_finnish_harmony_lemma.txt",
}

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
statistics = {name: load_statistics(path) for name, path in stats_files.items()}

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å
statistics = {k: v for k, v in statistics.items() if v}

# –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
labels = list(statistics.keys())
harmonic_values = [statistics[label].get("–ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞", 0) for label in labels]
disharmonic_values = [statistics[label].get("–ù–∞—Ä—É—à–∞—é—â–∏–µ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º", 0) for label in labels]

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
x = np.arange(len(labels))
width = 0.4
fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, harmonic_values, width, label='–ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞')
rects2 = ax.bar(x + width/2, disharmonic_values, width, label='–ù–∞—Ä—É—à–∞—é—â–∏–µ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º')

ax.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤')
ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –¥–æ –∏ –ø–æ—Å–ª–µ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏')
ax.set_xticks(x)
ax.set_xticklabels(labels, rotation=20, ha="right")
ax.legend()

plt.show()


# –ê–ù–ê–õ–ò–ó –ë–ò–ì–†–ê–ú–ú –ò –¢–†–ò–ì–†–ê–ú–ú


import numpy as np

def calculate_average_word_length(file_path):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Å–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            words = [word for line in file for word in line.strip().split() if word.isalpha()]
        avg_length = np.mean([len(word) for word in words]) if words else 0
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ –≤ {file_path}: {avg_length:.2f}")
        return avg_length
    except FileNotFoundError:
        print(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return None

# –§–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
files = ["clean_pure_text.txt", "clean_loanwords_text.txt"]

# –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
for file in files:
    calculate_average_word_length(file)

from collections import Counter

def extract_vowel_ngrams(words, vowels, n=2):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∏–≥—Ä–∞–º–º—ã –∏–ª–∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã –≥–ª–∞—Å–Ω—ã—Ö –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤"""
    ngram_counts = Counter()

    for word in words:
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–∞—Å–Ω—ã–µ
        word_vowels = [ch for ch in word if ch in vowels]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∏–≥—Ä–∞–º–º—ã –∏–ª–∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        ngrams = ["".join(word_vowels[i:i+n]) for i in range(len(word_vowels) - (n-1))]

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å—á–µ—Ç—á–∏–∫
        ngram_counts.update(ngrams)

    return ngram_counts

def find_harmony_violations(ngram_counts, front_vowels, back_vowels):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã, –Ω–∞—Ä—É—à–∞—é—â–∏–µ —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º"""
    violations = {}

    for ngram, count in ngram_counts.items():
        has_front = any(ch in front_vowels for ch in ngram)
        has_back = any(ch in back_vowels for ch in ngram)

        if has_front and has_back:
            violations[ngram] = count  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–∞—Ä—É—à–∏—Ç–µ–ª–µ–π

    return violations

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
words1 = all_words("clean_pure_text.txt")
words2 = all_words("clean_loanwords_text.txt")
vowels = set(all_vowels("finnish_vowels.txt"))

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä—è–¥—ã –≥–ª–∞—Å–Ω—ã—Ö
front_vowels = {"√§", "√∂", "y"}
back_vowels = {"a", "o", "u"}

# –ê–Ω–∞–ª–∏–∑ –±–∏–≥—Ä–∞–º–º –∏ —Ç—Ä–∏–≥—Ä–∞–º–º
bigrams1 = extract_vowel_ngrams(words1, vowels, n=2)
bigrams2 = extract_vowel_ngrams(words2, vowels, n=2)

trigrams1 = extract_vowel_ngrams(words1, vowels, n=3)
trigrams2 = extract_vowel_ngrams(words2, vowels, n=3)

# –ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞
harmony_violations_bigrams1 = find_harmony_violations(bigrams1, front_vowels, back_vowels)
harmony_violations_bigrams2 = find_harmony_violations(bigrams2, front_vowels, back_vowels)

harmony_violations_trigrams1 = find_harmony_violations(trigrams1, front_vowels, back_vowels)
harmony_violations_trigrams2 = find_harmony_violations(trigrams2, front_vowels, back_vowels)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\nüîπ **–¢–û–ü-10 –±–∏–≥—Ä–∞–º–º –≤ –¢–µ–∫—Å—Ç–µ 1**:", bigrams1.most_common(10))
print("üîπ **–¢–û–ü-10 –±–∏–≥—Ä–∞–º–º –≤ –¢–µ–∫—Å—Ç–µ 2**:", bigrams2.most_common(10))

print("\nüîπ **–¢–û–ü-10 —Ç—Ä–∏–≥—Ä–∞–º–º –≤ –¢–µ–∫—Å—Ç–µ 1**:", trigrams1.most_common(10))
print("üîπ **–¢–û–ü-10 —Ç—Ä–∏–≥—Ä–∞–º–º –≤ –¢–µ–∫—Å—Ç–µ 2**:", trigrams2.most_common(10))

print("\n‚ö†Ô∏è **–ë–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –≤ –¢–µ–∫—Å—Ç–µ 1**:", harmony_violations_bigrams1)
print("‚ö†Ô∏è **–ë–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –≤ –¢–µ–∫—Å—Ç–µ 2**:", harmony_violations_bigrams2)

print("\n‚ö†Ô∏è **–¢—Ä–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –≤ –¢–µ–∫—Å—Ç–µ 1**:", harmony_violations_trigrams1)
print("‚ö†Ô∏è **–¢—Ä–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –≤ –¢–µ–∫—Å—Ç–µ 2**:", harmony_violations_trigrams2)

import matplotlib.pyplot as plt

def plot_top_ngrams(ngram_counts, title, n=10):
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ —Ç–æ–ø-N –±–∏–≥—Ä–∞–º–º –∏–ª–∏ —Ç—Ä–∏–≥—Ä–∞–º–º"""
    top_ngrams = ngram_counts.most_common(n)
    labels, values = zip(*top_ngrams)

    plt.figure(figsize=(10, 5))
    plt.bar(labels, values, color="skyblue")
    plt.xlabel("–ë–∏–≥—Ä–∞–º–º—ã/–¢—Ä–∏–≥—Ä–∞–º–º—ã")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.title(title)
    plt.xticks(rotation=45)
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–≥—Ä–∞–º–º
plot_top_ngrams(bigrams1, "–¢–æ–ø-10 –±–∏–≥—Ä–∞–º–º –≤ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ")
plot_top_ngrams(bigrams2, "–¢–æ–ø-10 –±–∏–≥—Ä–∞–º–º –≤ –ó–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∏–≥—Ä–∞–º–º
plot_top_ngrams(trigrams1, "–¢–æ–ø-10 —Ç—Ä–∏–≥—Ä–∞–º–º –≤ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ")
plot_top_ngrams(trigrams2, "–¢–æ–ø-10 —Ç—Ä–∏–≥—Ä–∞–º–º –≤ –ó–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö")


def plot_violations(violations, title):
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ –±–∏–≥—Ä–∞–º–º/—Ç—Ä–∏–≥—Ä–∞–º–º, –Ω–∞—Ä—É—à–∞—é—â–∏—Ö —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º"""
    if not violations:
        print(f"‚úÖ –ù–µ—Ç –Ω–∞—Ä—É—à–µ–Ω–∏–π —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ –≤ {title}")
        return

    labels, values = zip(*sorted(violations.items(), key=lambda x: x[1], reverse=True)[:10])

    plt.figure(figsize=(10, 5))
    plt.bar(labels, values, color="red")
    plt.xlabel("–ë–∏–≥—Ä–∞–º–º—ã/–¢—Ä–∏–≥—Ä–∞–º–º—ã")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.title(title)
    plt.xticks(rotation=45)
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Ä—É—à–µ–Ω–∏–π —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞
plot_violations(harmony_violations_bigrams1, "–ë–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ (–¢–µ–∫—Å—Ç 1)")
plot_violations(harmony_violations_bigrams2, "–ë–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ (–¢–µ–∫—Å—Ç 2)")

plot_violations(harmony_violations_trigrams1, "–¢—Ä–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ (–¢–µ–∫—Å—Ç 1)")
plot_violations(harmony_violations_trigrams2, "–¢—Ä–∏–≥—Ä–∞–º–º—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —Å–∏–Ω–≥–∞—Ä–º–æ–Ω–∏–∑–º–∞ (–¢–µ–∫—Å—Ç 2)")


–í—ã–≤–æ–¥—ã –ø–æ –ø—Ä–æ–µ–∫—Ç—É: 
